<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MORE3D</title>
<!--  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">-->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Multimodal 3D Reasoning Segmentation with Complex Scenes</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://jiang-xueying.github.io/" target="_blank">Xueying Jiang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=zdgKJXIAAAAJ&hl=en" target="_blank">Lewei Lu</a><sup>2</sup>,</span>
              <span class="author-block">
                    <a href="https://ling-shao.github.io/" target="_blank">Ling Shao</a><sup>3</sup>,</span>
              <span class="author-block">
                    <a href="https://personal.ntu.edu.sg/shijian.lu/" target="_blank">Shijian Lu</a><sup>1*</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>S-Lab, Nanyang Technological University, Singapore</span><br>
                    <span class="author-block"><sup>2</sup>Sensetime Research, China</span><br>
                    <span class="author-block"><sup>3</sup>UCAS-Terminus AI Lab, University of Chinese Academy of Sciences, China</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author.</small></span>
                    <br>
                    <span class="author-block"><strong>Preprint</strong></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2411.13927.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                            <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2411.13927" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>

<!--                    &lt;!&ndash; Supplementary PDF link &ndash;&gt;-->
<!--                    <span class="link-block">-->
<!--                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"-->
<!--                      class="external-link button is-normal is-rounded is-dark">-->
<!--                      <span class="icon">-->
<!--                        <i class="fas fa-file-pdf"></i>-->
<!--                      </span>-->
<!--                      <span>Supplementary</span>-->
<!--                    </a>-->
<!--                  </span>-->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Dataset (Coming Soon)</span>
                </a>
              </span>


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/motivation.jpg" class="center-image blend-img-background"/>
      <h2 class="subtitle has-text-justified">
        The proposed MORE3D enables multi-object reasoning segmentation for 3D scenarios. It can comprehend the intention behind user questions, handle complex 3D scenes with multiple objects, and produce fine-grained explanations with 3D spatial relations among objects, demonstrating strong reasoning and 3D segmentation capabilities.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The recent development in multimodal learning has greatly advanced the research in 3D scene understanding in various real-world tasks such as embodied AI. However, most existing work shares two typical constraints: 1) they are short of reasoning ability for interaction and interpretation of human intension and 2) they focus on scenarios with single-category objects only which leads to over-simplified textual descriptions due to the negligence of multi-object scenarios and spatial relations among objects. We bridge the research gaps by proposing a 3D reasoning segmentation task for multiple objects in scenes. The task allows producing 3D segmentation masks and detailed textual explanations as enriched by 3D spatial relations among objects. To this end, we create ReasonSeg3D, a large-scale and high-quality benchmark that integrates 3D segmentation masks and 3D spatial relations with generated question-answer pairs. In addition, we design MORE3D, a novel 3D reasoning network that works with queries of multiple objects and tailored 3D scene understanding designs. MORE3D learns detailed explanations on 3D relations and employs them to capture spatial information of objects and reason textual outputs. Extensive experiments show that MORE3D excels in reasoning and segmenting complex multi-object 3D scenes, and the created ReasonSeg3D offers a valuable platform for future exploration of 3D reasoning segmentation. The dataset and code will be released.


          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->




<section class="section hero is-small">
<div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full">
    <div class="content">
      <h2 class="title is-3">ReasonSeg3D Dataset Generation</h2>
      <center>
      <img src="static/images/dataset_generation_pipeline.jpg" style="max-width: 95%; height: auto;" alt="Illustration of the prompt template in our dataset generation." class="center-image blend-img-background"/>
      </center>
      <div class="level-set has-text-justified">
        <p>
           Illustration of the prompt template in our dataset generation. (a) One example prompt template in our dataset generation on 3D multi-object reasoning. (b) With a sample input image to GPT-4o and the corresponding ground-truth segmentation on the top, the two boxes below present one generated question-answer pair where text of different colors highlights different objects.
        </p>
      </div>
    </div>
  </div>
</div>
</div>
</section>

<section class="section hero is-small is-light">
<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full">
      <div class="content">
        <h2 class="title is-3">Method Overview: Overall Framework of MORE3D</h2>
        <center>
        <img src="static/images/overall_architecture.jpg" style="max-width: 95%; height: auto;" alt="Overview of our proposed MORE3D." class="center-image blend-img-background"/>
        </center>
        <div class="level-set has-text-justified">
          <p>
             Overview of our proposed MORE3D: Given an input point cloud, the 3D Encoder first extracts per-point features and projects them into sequential features. The sequential features, together with the textual input, are then fed into a multimodal LLM to perform reasoning, producing textual answers with both detailed explanations and descriptions of 3D spatial relationships among multiple objects. Finally, embeddings for multiple <SEG> tokens and the per-point features are passed to the 3D Decoder to produce 3D segmentation masks and classification results. The module marked with a snowflake icon is frozen during training, while those marked with a flame icon are trainable.
          </p>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-small">
<div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full">
    <div class="content">
      <h2 class="title is-3">Multi-Object 3D Reasoning Segmentation</h2>
      <center>
      <img src="static/images/object_specific_feature_extraction.jpg" style="max-width: 80%; height: auto;" alt="Extraction of object-specific point cloud embeddings." class="center-image blend-img-background"/>
      </center>
      <div class="level-set has-text-justified">
        <p>
           Extraction of object-specific point cloud embeddings. Each predicted textual answer contains multiple <SEG> tokens, with their positions recorded in a multi-segment index list. The LLM embedding corresponding to each <SEG> token is extracted based on the multi-segment index list for obtaining object-specific point cloud embeddings.
        </p>
      </div>
    </div>
  </div>
</div>
</div>
</section>


<section class="section hero is-small is-light">
<div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full">
    <div class="content">
      <h2 class="title is-3">Segmentation Visualization</h2>
      <center>
      <img src="static/images/visualization.jpg" style="max-width: 95%; height: auto;" alt="Segmentation visualization over the ReasonSeg3D validation set." class="center-image blend-img-background"/>
      </center>
      <div class="level-set has-text-justified">
        <p>
           Segmentation visualization over the ReasonSeg3D validation set. Each case presents a user input question, the corresponding input point cloud, the ground-truth segmentation, and the prediction by the proposed MORE3D. Best viewed in color and zoom-in. Green indicates chairs, and pink indicates tables.
        </p>
      </div>
    </div>
  </div>
</div>
</div>
</section>

  <section class="section hero is-small" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <div class="box">
      <pre><code>
          @article{jiang2024multimodal,
            title={Multimodal 3D Reasoning Segmentation with Complex Scenes},
            author={Jiang, Xueying and Lu, Lewei and Shao, Ling and Lu, Shijian},
            journal={arXiv preprint arXiv:2411.13927},
            year={2024}
          }
      </code></pre>
    </div>
  </div>
</section>

<!--&lt;!&ndash;BibTex citation &ndash;&gt;-->
<!--  <section class="section hero is-small" id="BibTeX">-->
<!--    <div class="container is-max-desktop content">-->
<!--      <h2 class="title">BibTeX</h2>-->
<!--      <pre><code>-->
<!--          @article{jiang2024multimodal,-->
<!--            title={Multimodal 3D Reasoning Segmentation with Complex Scenes},-->
<!--            author={Jiang, Xueying and Lu, Lewei and Shao, Ling and Lu, Shijian},-->
<!--            journal={arXiv preprint arXiv:2411.13927},-->
<!--            year={2024}-->
<!--          }-->
<!--      </code></pre>-->
<!--    </div>-->
<!--</section>-->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
