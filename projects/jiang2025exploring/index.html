<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SHARP</title>
<!--  下面这个icon要记得注释掉-->
<!--  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">-->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Exploring 3D Activity Reasoning and Planning: From Implicit Human Intentions to Route-Aware Planning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://jiang-xueying.github.io/" target="_blank">Xueying Jiang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=U5Y7BjkAAAAJ&hl=zh-CN" target="_blank">Wenhao Li</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://ieeexplore.ieee.org/author/37405025600" target="_blank">Xiaoqin Zhang</a><sup>2</sup>,</span>
              <span class="author-block">
                    <a href="https://ling-shao.github.io/" target="_blank">Ling Shao</a><sup>3</sup>,</span>
              <span class="author-block">
                    <a href="https://personal.ntu.edu.sg/shijian.lu/" target="_blank">Shijian Lu</a><sup>1*</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>College of Computing and Data Science, Nanyang Technological University, Singapore</span><br>
                    <span class="author-block"><sup>2</sup>College of Computer Science and Technology, Zhejiang University of Technology, China</span><br>
                    <span class="author-block"><sup>3</sup>UCAS-Terminus AI Lab, University of Chinese Academy of Sciences, China</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author.</small></span>
                    <br>
                    <span class="author-block"><strong>Preprint</strong></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2503.12974" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                            <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2503.12974" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>



                  <!-- Github link -->
                  <span class="link-block">
                    <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Dataset (Coming Soon)</span>
                </a>
              </span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/motivation.jpg" class="center-image blend-img-background"/>
      <h2 class="subtitle has-text-justified">
        The proposed 3D Activity Reasoning and Planning enables reasoning activities underneath user’s implicit instructions. It can generate detailed executable steps for the reasoned activities within 3D scenes, as well as consistent inter-step route planning with object shapes and object locations from fine-grained 3D segmentation.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            3D activity reasoning and planning has attracted increasing attention in human-robot interaction and embodied AI thanks to the recent advance in multimodal learning. However, most existing works share two constraints: 1) heavy reliance on explicit instructions with little reasoning on implicit user intention; 2) negligence of inter-step route planning on robot moves. To bridge the gaps, we propose 3D activity reasoning and planning, a novel 3D task that reasons the intended activities from implicit instructions and decomposes them into steps with inter-step routes and planning under the guidance of fine-grained 3D object shapes and locations from scene segmentation. We tackle the new 3D task from two perspectives. First, we construct ReasonPlan3D, a large-scale benchmark that covers diverse 3D scenes with rich implicit instructions and detailed annotations for multi-step task planning, inter-step route planning, and fine-grained segmentation. Second, we design a novel framework that introduces progressive plan generation with contextual consistency across multiple steps, as well as a scene graph that is updated dynamically for capturing critical objects and their spatial relations. Extensive experiments demonstrate the effectiveness of our benchmark and framework in reasoning activities from implicit human instructions, producing accurate stepwise task plans, and seamlessly integrating route planning for multi-step moves. The dataset and code will be released.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section hero is-small">
<div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full">
    <div class="content">
      <h2 class="title is-3">ReasonPlan3D Benchmark</h2>
      <center>
      <img src="static/images/dataset_statistics.jpg" style="max-width: 95%; height: auto;" class="center-image blend-img-background"/>
      </center>
      <div class="level-set has-text-justified">
        <p>
           Benchmark statistics. The pie chart in (a) shows the most frequently occurring verbs in inter-step route planning, along with their associated adverbs representing movements. The bar charts in (b) and (c) present actions and their associated objects in the step-by-step plans, and that in (d) show the distribution of answers across different step counts.
        </p>
      </div>
    </div>
  </div>
</div>
</div>
</section>


<section class="section hero is-small is-light">
<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full">
      <div class="content">
        <h2 class="title is-3">Method Overview: Overall Framework of SHARP</h2>
        <center>
        <img src="static/images/overall_architecture.jpg" style="max-width: 95%; height: auto;" class="center-image blend-img-background"/>
        </center>
        <div class="level-set has-text-justified">
          <p>
             Overall architecture of the proposed SHARP method. Given the point clouds of a 3D scene, the Point Cloud Encoder generates scene-level embeddings, while the 3D segmentor predicts 3D object masks. Besides, the 2D Encoder extracts multi-view image features, which are combined with 3D object masks as input to the Scene Graph Generator to obtain scene graph. The generated scene graph is then fed into the Graph Encoder, together with the scene-level embedding as the inputs of the MLLM. For step s, the one-step plan is generated by referring to previous steps, and the scene graph is updated by graph modulation weights that emphasize objects and their spatial relations that are critical to the reasoned activity. The snow icon indicates frozen modules, while the fire icon indicates trainable modules.
          </p>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

  <section class="section hero is-small">
<div class="container is-max-desktop">
<div class="columns is-centered">
  <div class="column is-full">
    <div class="content">
      <h2 class="title is-3">Visualization</h2>
      <center>
      <img src="static/images/visualization_predictions.jpg" style="max-width: 95%; height: auto;" class="center-image blend-img-background"/>
      </center>
      <div class="level-set has-text-justified">
        <p>
           Activity reasoning and planning visualization over the ReasonPlan3D val set. Each example shows an implicit human instruction, the input point clouds of the 3D scene, the 3D segmentation of the scene, and the predictions from 3DGraphLLM and the proposed SHARP. Best viewed in color and zoom-in.
        </p>
      </div>
    </div>
  </div>
</div>
</div>
</section>

  <section class="section hero is-small is-light" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <div class="box">
      <pre><code>
@article{jiang2025exploring,
    ...
}
      </code></pre>
    </div>
  </div>
</section>

  <!--BibTex citation -->
  <section class="section hero is-small is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
          @article{jiang2025exploring,
            title={Exploring 3D Activity Reasoning and Planning: From Implicit Human Intentions to Route-Aware Planning},
            author={Jiang, Xueying and Li, Wenhao and Zhang, Xiaoqin and Shao, Ling and Lu, Shijian},
            journal={arXiv preprint arXiv:2503.12974},
            year={2025}
          }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
